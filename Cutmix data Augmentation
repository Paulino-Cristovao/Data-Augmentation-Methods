#https://arxiv.org/pdf/1905.04899.pdf
# for theoretical explanation see the paper.



# training
def train(train_loader, model, criterion, optimizer, epoch):
    losses = AverageMeter()
    top1 = AverageMeter()

    # switch to train mode
    model.train()

    for batch_idx, data in enumerate(train_loader):
        optimizer.zero_grad()
        
        # pass data to cuda
        input = data[0].to(device)
        label = data[1].to(device)        
        
        # cutmix
        r = np.random.rand(1)
        beta_ = 1
        if 1 > 0 and r < 0.5:
            # generate mixed sample
            lam = np.random.beta(beta_, beta_)
            rand_index = torch.randperm(input.size()[0]).to(device)
            target_a = label
            target_b = label[rand_index]
            bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)
            input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]
            # adjust lambda to exactly match pixel ratio
            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))
            # compute output
            output,_ = model(input)
            loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)
        
        else:
            # compute output
            output,_ = model(input)
            loss = criterion(output,label)


        prec1, _ = accuracy(output, label, topk=(1, 5))
        losses.update(loss.item())
        top1.update(prec1.item(), input.size(0))
   
        # compute gradient and do SGD step
        loss.backward()
        optimizer.step()

        model.weight_norm()

        # plot progress
        if batch_idx == len(train_loader) :  
          print('({batch}/{size}) |  Loss: {loss:.4f} | top1: {top1: .4f}'.format(
                      batch=batch_idx + 1,
                      size=len(train_loader),
                      loss=losses.avg,
                      top1=top1.avg,
                      ))
    return (losses.avg, top1.avg, model)
