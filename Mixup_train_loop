

def mixup(data, targets, alpha, n_classes):
    indices = torch.randperm(data.size(0))
    data2 = data[indices]
    targets2 = targets[indices]

    targets = onehot(targets, n_classes)
    targets2 = onehot(targets2, n_classes)

    lam = torch.FloatTensor([np.random.beta(alpha, alpha)])
    data = data * lam + data2 * (1 - lam)
    targets = targets * lam + targets2 * (1 - lam)

    return data, targets
    
    
    
### at training loop


def train(train_loader, model, criterion, optimizer, epoch):
    losses = AverageMeter()
    top1 = AverageMeter()

    # switch to train mode
    model.train()

    for batch_idx, data in enumerate(train_loader):
        optimizer.zero_grad()
        
        # mixUp
        alpha = 1
        image, target = mixup(data[0], data[1], alpha, base_class)
        input = image.to(device)
        label = target.to(device)
        label = torch.max(label, 1)[1]
        
        
        # compute output
        output,_ = model(input)
        loss = criterion(output,label)


        prec1, _ = accuracy(output, label, topk=(1, 5))
        losses.update(loss.item())
        top1.update(prec1.item(), input.size(0))
   
        # compute gradient and do SGD step
        loss.backward()
        optimizer.step()

        model.weight_norm()

        # plot progress
        if batch_idx == len(train_loader) :  
          print('({batch}/{size}) |  Loss: {loss:.4f} | top1: {top1: .4f}'.format(
                      batch=batch_idx + 1,
                      size=len(train_loader),
                      loss=losses.avg,
                      top1=top1.avg,
                      ))
    return (losses.avg, top1.avg, model)
